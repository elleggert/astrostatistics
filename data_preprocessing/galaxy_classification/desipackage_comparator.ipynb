{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### File to evaluate difference in utilising isLRG directly and the full DESI pipeline\n",
    "\n",
    "##### Existing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from astropy.io import fits\n",
    "import os\n",
    "import wget\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from brick import Brick\n",
    "import telegram_send\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" File to download, process, classify and delete galaxies from DR9 all in one\"\"\"\n",
    "\n",
    "\"\"\" Defining area to download, how many bricks to download in one session and which storage to use (Astrodisk is the name of a hardrive)\"\"\"\n",
    "area = 'south'\n",
    "device = 'Astrodisk'\n",
    "bricks_to_classify = 30000\n",
    "south_survey_is_south =  True\n",
    "\n",
    "## ToDo: Create special folder in astrodisk with dedicated bricks\n",
    "hdulistBricksSouthSummary = fits.open('../../bricks_data/survey-bricks-dr9-south.fits')\n",
    "data_south = hdulistBricksSouthSummary[1].data\n",
    "brickname_south = data_south.field('brickname')\n",
    "brickid_south = data_south.field('brickid')\n",
    "south_survey_is_south = data_south.field('survey_primary')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Everything is times in order to measure when the pipeline slows down\n",
    "start = time.time()\n",
    "\n",
    "print()\n",
    "print(f\"=============================== Process {area} ..... ==================================\")\n",
    "print()\n",
    "\n",
    "bricks_name = []\n",
    "bricks_path = []\n",
    "\n",
    "# Getting already downloaded files from the Harddrive:\n",
    "for filename in os.listdir(f'/Volumes/{device}/bricks_data/{area}/'):\n",
    "    brickn = filename.replace(\"tractor-\", \"\")\n",
    "    brickn = brickn.replace(\".fits\", \"\")\n",
    "    bricks_path.append(filename)\n",
    "    bricks_name.append(brickn)\n",
    "\n",
    "\n",
    "# Define empty Dataframes that will hold the information on stars and galaxies\n",
    "df_galaxy = pd.DataFrame(columns=['BrickID', 'RA', 'DEC', 'LRG', 'ELG', 'QSO'])\n",
    "df_stars = pd.DataFrame(columns=['RA', 'DEC', 'GMAG', 'RMAG', 'ZMAG'])\n",
    "\n",
    "\n",
    "\n",
    "# Prints information on the  current session e.g. how many bricks are left --> all the code until here takes a few minutes to complete\n",
    "print(f\"No of bricks to classify in {area}: {len(bricks_name)} \")\n",
    "print(\"Time taken for bricks left extraction: \", round(((time.time() - start) / 60), 2))\n",
    "\n",
    "\n",
    "# There have been problems with very few bricks that were not found on the servers, this code is only to avoid the script from crashing here\n",
    "c = 0\n",
    "problem_bricks = []\n",
    "inter = time.time()\n",
    "\n",
    "\n",
    "# This is the actual loop doing the classification for the bricks that are missing from the catalogue:\n",
    "\n",
    "for i, brickname in enumerate(bricks_name):\n",
    "\n",
    "    # Download Brick\n",
    "\n",
    "\n",
    "\n",
    "    brickid = brickid_south[np.where(brickname_south == brickname)]\n",
    "\n",
    "    # North Bricks\n",
    "    # brickid = brickid_north[np.where(brickname_north == brickname)]\n",
    "\n",
    "    if len(brickid > 0):\n",
    "        brickid = brickid[0]\n",
    "    else:\n",
    "        brickid = 0\n",
    "\n",
    "    # Open Brick\n",
    "\n",
    "    hdu = fits.open(f'/Volumes/{device}/bricks_data/{area}/tractor-{brickname}.fits')\n",
    "    data = hdu[1].data\n",
    "\n",
    "    # Define the Brick Object  --> in brick.py\n",
    "    brick = Brick(data)\n",
    "\n",
    "    # south = north_survey_is_south[np.where(brickid_north == brickid)]\n",
    "\n",
    "    south = south_survey_is_south[np.where(brickid_south == brickid)]\n",
    "    if len(south) > 0:\n",
    "        south = south[0]\n",
    "    else:\n",
    "        south = True\n",
    "\n",
    "    ## Enable this is classifying North Objects\n",
    "    # south = north_survey_is_south[np.where(brickid_north == brickid)][0]\n",
    "\n",
    "    # Initialise Brick Object\n",
    "    brick.initialise_brick_for_galaxy_classification(south)\n",
    "\n",
    "    # Classify Brick objects into categories --> takes under 1 second after optimisation\n",
    "    target_objects = brick.classify_galaxies()\n",
    "\n",
    "    # Appending one empty line per brick to be sure that all bricks are extracted\n",
    "    df_galaxy = df_galaxy.append({'BrickID': brickid, 'RA': np.nan, 'DEC': np.nan, 'LRG': 0, 'ELG': 0, 'QSO': 0},\n",
    "                                 ignore_index=True)\n",
    "\n",
    "    support_df = pd.DataFrame(target_objects,\n",
    "                              columns=['BrickID', 'RA', 'DEC', 'LRG', 'ELG', 'QSO'])\n",
    "\n",
    "    df_galaxy = df_galaxy.append(support_df)\n",
    "\n",
    "    # Repeat steps for stellar objects\n",
    "\n",
    "    brick.initialise_brick_for_stellar_density()\n",
    "\n",
    "    stars = brick.get_stellar_objects()\n",
    "\n",
    "    support_df = pd.DataFrame(stars, columns=['RA', 'DEC', 'GMAG', 'RMAG', 'ZMAG'])\n",
    "    df_stars = df_stars.append(support_df)\n",
    "\n",
    "    # Every 100 objects, the newly classified objects are added to the existing catalogue to avoid massive reruns when the script crashes\n",
    "\n",
    "    \"\"\"if i % 100 == 0:\n",
    "        print()\n",
    "        print(i / (bricks_to_classify / 100), '%')\n",
    "        df_galaxy = df_galaxy.astype(\n",
    "            {'BrickID': 'int32', 'LRG': 'int8', 'ELG': 'int8', 'QSO': 'int8'})\n",
    "        df_galaxy.to_csv(f'../../bricks_data/galaxy_catalogue_{area}.csv', mode='a', index=False, header=False)\n",
    "        df_stars.to_csv(f'../../bricks_data/stellar_catalogue_{area}.csv', mode='a', index=False, header=False)\n",
    "        # df_galaxy.to_csv('../../bricks_data/galaxy_catalogue_sample_profiling.csv', index=False, header=False)\n",
    "        # df_stars.to_csv('../../bricks_data/stellar_catalogue_sample_profiling.csv', index=False, header=False)\n",
    "        df_galaxy = df_galaxy[0:0]\n",
    "        df_stars = df_stars[0:0]\"\"\"\n",
    "\n",
    "    # This script used to send me updates to my phone using a Telegram Bot, so i knew when it crashed or it was completed\n",
    "\n",
    "    # Remove Downloaded Brick\n",
    "    # os.remove(f'/Volumes/{device}/bricks_data/{area}/tractor-{brickname}.fits')\n",
    "\n",
    "    # Stop the loop when the defined number of bricks was classified, if this number is greater than remaining bricks, script will run till all bricks are finished\n",
    "    if i > bricks_to_classify:\n",
    "        break\n",
    "\n",
    "    print(f\" Brick {area} processed: \", brickname, \", Brick \", i, \" of \", bricks_to_classify)\n",
    "\n",
    "df_galaxy = df_galaxy.astype(\n",
    "    {'BrickID': 'int32', 'LRG': 'int8', 'ELG': 'int8', 'QSO': 'int8'})\n",
    "#df_galaxy.to_csv(f'../../bricks_data/galaxy_catalogue_{area}.csv', mode='a', index=False, header=False)\n",
    "#df_stars.to_csv(f'../../bricks_data/stellar_catalogue_{area}.csv', mode='a', index=False, header=False)\n",
    "#df_galaxy = df_galaxy[0:0]\n",
    "#df_stars = df_stars[0:0]\n",
    "print()\n",
    "print(f\"=============================== Download {area} completed ==================================\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Prints session statistics upon completion\n",
    "print(\"Hours taken for: \", bricks_to_classify, \" bricks: \", round(((time.time() - start) / 3600), 2))\n",
    "# message = f'++++++ Finished {bricks_to_classify} bricks. Avg. Bandwidths: {round(((time.time() - start) / bricks_to_classify), 2)} seconds per brick ++++++'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}