{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## First Attempt at building a deepsets architecture"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from models.deep_set import utilities\n",
    "from set_dataloader import CCD\n",
    "import time\n",
    "\n",
    "# Import NN Packages\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing, metrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building a deepsets architecture\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing Dataset Utility to pass data in the right format\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Build a NN sampling an equal number of CCDs per 256 pixel and pass through deep sets for regression\n",
    "\n",
    "2. Adapt NN for variable sized inputs\n",
    "\n",
    "3. Adapt NN to use 64 inputs of size 2048 to then predict density at 256\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "' Todo\\n1. Where to get the data from\\n2. Scaling --> import an already scaled dataset, this will have to be prepared but should be same for Neural Net\\n3. Combine larger and smaller dataset\\n4. Build 64 input channels instead of one, so one more dimension of tensors( NO of Pixels,no_of_subpixels,no_ccds, no_features)\\n'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noinspection PyAttributeOutsideInit\n",
    "class SetSequence(Dataset):\n",
    "    \"\"\"Processes and Returns a Dataset of Variable Sized Input Sets of Dimensions\n",
    "    N = Number Pixels of that are returned\n",
    "    M = Max Size of each Individual Set of CCDs\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pixels=10, max_ccds=30, var_set_len=False):\n",
    "\n",
    "        with open('../../bricks_data/pixel2ccd_256_non_inclusive.pickle', 'rb') as f:\n",
    "            self.pixel2ccd_dict = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "        self.ccd = CCD()\n",
    "        self.num_features = self.ccd.num_features\n",
    "\n",
    "        # Dimensions\n",
    "        self.num_pixels = num_pixels\n",
    "        self.max_ccds = max_ccds\n",
    "        self.var_set_len = var_set_len\n",
    "\n",
    "        df_raw = pd.read_csv('../../bricks_data/dataset_geometric.csv')\n",
    "        # Randomly Sampling Pixel Indices from Dataframe\n",
    "        pixel_indices = random.sample(range(len(df_raw)), num_pixels)\n",
    "\n",
    "        self.df = df_raw.iloc[pixel_indices]\n",
    "        self.pix_ids = self.df.pixel_id.to_numpy()\n",
    "\n",
    "        self.initialise_inputs()\n",
    "\n",
    "        self.initialise_lengths()\n",
    "\n",
    "        # Target\n",
    "        self.label = np.random.rand(self.num_pixels * self.max_ccds)\n",
    "\n",
    "        # Mask Variable Len Sets\n",
    "        #self.set_max_set_len()\n",
    "\n",
    "    def set_targets(self, gal_type):\n",
    "        # Features and inputs:\n",
    "        self.target = None\n",
    "        self.target = self.df[gal_type].to_numpy()\n",
    "        #print(self.target.shape)\n",
    "        self.scaler_out = preprocessing.MinMaxScaler()\n",
    "        self.target = self.scaler_out.fit_transform(self.target.reshape(-1, 1))\n",
    "        #print(self.target.shape)\n",
    "\n",
    "    def initialise_lengths(self):\n",
    "        self.lengths = np.zeros(self.num_pixels, dtype=int)\n",
    "        if self.var_set_len:\n",
    "            for i, pix in enumerate(self.pix_ids):\n",
    "                c = len(self.pixel2ccd_dict[pix])\n",
    "                if c < self.max_ccds:\n",
    "                    self.lengths[i] = c\n",
    "                else:\n",
    "                    self.lengths[i] = self.max_ccds\n",
    "\n",
    "        else:\n",
    "            self.lengths.fill(self.max_ccds)\n",
    "\n",
    "    def initialise_inputs(self):\n",
    "        #self.input = -1 * np.ones((self.num_pixels, self.max_ccds, self.num_features))\n",
    "        self.input = np.zeros((self.num_pixels, self.max_ccds, self.num_features))\n",
    "\n",
    "        # Iterate through the pixels\n",
    "        for i, pix in enumerate(self.pix_ids):\n",
    "            ids = self.pixel2ccd_dict[pix]\n",
    "            random.shuffle(ids)\n",
    "            #print(len(ids))\n",
    "            ids = ids[:self.max_ccds]\n",
    "            #print(len(ids))\n",
    "            #print()\n",
    "            x = self.ccd.get_ccds(ids)\n",
    "            # Iterate through the CCDs for every pixel\n",
    "            for j in range(len(ids)):\n",
    "                self.input[i, j] = x[j]\n",
    "\n",
    "    def set_max_set_len(self):\n",
    "        self.index_matrix = -1 * np.ones((self.num_pixels, self.max_ccds), dtype=int)\n",
    "\n",
    "        # Getting random labels for now, in the future this will be the output densities\n",
    "\n",
    "        m = 0\n",
    "        for i in range(self.num_pixels):\n",
    "\n",
    "            for j in range(self.lengths[i]):\n",
    "                ''' This code with label == 0 is not yet needed, but this masking will become necessary when I have\n",
    "                    I have 64 subpixels per pixel and some of those are not covered by CCDs'''\n",
    "                while self.label[m] == 0:\n",
    "                    m += 1\n",
    "                self.index_matrix[i, j] = m\n",
    "                m += 1\n",
    "\n",
    "        print(self.lengths)\n",
    "        print(self.index_matrix)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_pixels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.input[idx]).float()\n",
    "        #x = x.unsqueeze(0)\n",
    "        y = torch.tensor(self.target[idx, 0]).float()\n",
    "        #print(y.shape)\n",
    "        y = y.unsqueeze(-1)\n",
    "        #print(y.shape)\n",
    "\n",
    "        #l = torch.tensor(self.lengths[idx])\n",
    "        l = self.lengths[idx]\n",
    "\n",
    "        return x, y, l\n",
    "\n",
    "\n",
    "\"\"\" Todo\n",
    "1. Where to get the data from\n",
    "2. Scaling --> import an already scaled dataset, this will have to be prepared but should be same for Neural Net\n",
    "3. Combine larger and smaller dataset\n",
    "4. Build 64 input channels instead of one, so one more dimension of tensors( NO of Pixels,no_of_subpixels,no_ccds, no_features)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "torch.Size([30, 9])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "traindata = SetSequence(var_set_len=True)\n",
    "traindata.set_targets('lrg')\n",
    "x, y, l = traindata.__getitem__(3)\n",
    "print(l.shape)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the Actual Network Architecture\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from deepset_layers import InvLinear\n",
    "class SetNet(nn.Module):\n",
    "    def __init__(self, n_features=5, n_output=3, reduction='sum'):\n",
    "        super(SetNet, self).__init__()\n",
    "\n",
    "        # Takes an Input Tensor and applies transformations to last layer --> features\n",
    "        # Output of Feature Layer: Tensor with Max.CCDs elements, which can now be passed to Set Layer\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(n_features, 7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(7, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(5, n_output),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.adder = InvLinear(3, 1, reduction=reduction, bias=True)\n",
    "\n",
    "        # Invariant Layer Influenced by Code from DPernes, but adapted for the current regression task instead of CNN\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        y = self.feature_extractor(X)\n",
    "\n",
    "        y = self.adder(y, mask=mask)\n",
    "        return y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10, 1)\n",
      "9\n",
      "tensor([[0.5417]])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "traindata = SetSequence(var_set_len=True)\n",
    "traindata.set_targets('lrg')\n",
    "x, y, l = traindata.__getitem__(3)\n",
    "print(l)\n",
    "print(y)\n",
    "print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVLAYER: torch.Size([1, 30, 3])\n",
      "tensor([[-8.7350]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = SetNet()\n",
    "y = net.forward(x)\n",
    "print(y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "#Work out masking logic\n",
    "device = 'cpu'\n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "l = l.to(device)\n",
    "\n",
    "\n",
    "def get_mask(sizes, max_size):\n",
    "    return (torch.arange(max_size).reshape(1, -1).to(sizes.device) < sizes.reshape(-1, 1))\n",
    "\n",
    "\n",
    "mask = get_mask(l, x.shape[1])\n",
    "print(mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Hyperparameters and Training Loops"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Defining Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#Defining Hyperparemeters\n",
    "no_epochs = 100  #very low, but computational power not sufficient for more iterations\n",
    "batch = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Using the Adam Method for Stochastic Optimisation\n",
    "#optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "galaxy_types = ['lrg', 'elg', 'qso']\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "def get_mask(sizes, max_size):\n",
    "    return (torch.arange(max_size).reshape(1, -1).to(sizes.device) < sizes.reshape(-1, 1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "traindata = SetSequence(num_pixels=1000, var_set_len=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GALAXY TYPE:  lrg\n",
      "\n",
      "Loss for Epoch 0 :  2942.1800325997174\n",
      "Loss for Epoch 10 :  3.680534098879434\n",
      "Loss for Epoch 20 :  2.884592419693945\n",
      "Loss for Epoch 30 :  2.765893154341029\n",
      "Loss for Epoch 40 :  2.5631455073598772\n",
      "Loss for Epoch 50 :  2.684168670588406\n",
      "Loss for Epoch 60 :  2.5554687872645445\n",
      "Loss for Epoch 70 :  2.4053838056570385\n",
      "Loss for Epoch 80 :  2.4015086796716787\n",
      "Loss for Epoch 90 :  2.399972525483463\n",
      "\n",
      "0.39641 minutes (23.8 seconds) taken to train the model\n",
      "\n",
      "GALAXY TYPE:  elg\n",
      "\n",
      "Loss for Epoch 0 :  27871.49012487009\n",
      "Loss for Epoch 10 :  8.458283477695659\n",
      "Loss for Epoch 20 :  4.994630630942993\n",
      "Loss for Epoch 30 :  4.833354047266766\n",
      "Loss for Epoch 40 :  4.84361100976821\n",
      "Loss for Epoch 50 :  4.83717059326591\n",
      "Loss for Epoch 60 :  4.820220751920715\n",
      "Loss for Epoch 70 :  4.846069120801985\n",
      "Loss for Epoch 80 :  4.8225392025779\n",
      "Loss for Epoch 90 :  4.808886545091809\n",
      "\n",
      "0.42237 minutes (25.3 seconds) taken to train the model\n",
      "\n",
      "GALAXY TYPE:  qso\n",
      "\n",
      "Loss for Epoch 0 :  14189.960423341021\n",
      "Loss for Epoch 10 :  13.390178092289716\n",
      "Loss for Epoch 20 :  11.792379513382912\n",
      "Loss for Epoch 30 :  12.168904803926125\n",
      "Loss for Epoch 40 :  12.2246720418334\n",
      "Loss for Epoch 50 :  10.915961837163195\n",
      "Loss for Epoch 60 :  10.814511087955907\n",
      "Loss for Epoch 70 :  10.813041705056094\n",
      "Loss for Epoch 80 :  10.80818999488838\n",
      "Loss for Epoch 90 :  10.80691729253158\n",
      "\n",
      "0.38134 minutes (22.9 seconds) taken to train the model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for gal in galaxy_types:\n",
    "    model = SetNet(n_features=traindata.num_features, reduction='max').to(device)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(\"GALAXY TYPE: \", gal)\n",
    "    print()\n",
    "    traindata.set_targets(gal_type=gal)\n",
    "\n",
    "    time_start = time.time()\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "        loss_per_epoch = 0\n",
    "        #loading the training data from trainset and shuffling for each epoch\n",
    "        trainloader = torch.utils.data.DataLoader(traindata, batch_size=batch, shuffle=True)\n",
    "\n",
    "        for i, (X, labels, set_sizes) in enumerate(trainloader):\n",
    "            #Put Model into train mode\n",
    "            model.train()\n",
    "\n",
    "            #Extract inputs and associated labels from dataloader batch\n",
    "            X = X.to(device)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            set_sizes = set_sizes.to(device)\n",
    "\n",
    "            mask = get_mask(set_sizes, X.shape[1])\n",
    "\n",
    "            #Predict outputs (forward pass)\n",
    "\n",
    "            predictions = model(X, mask=mask)\n",
    "\n",
    "            #Compute Loss\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            #Zero-out the gradients before backward pass (pytorch stores the gradients)\n",
    "            optimiser.zero_grad()\n",
    "            #Backpropagation\n",
    "            loss.backward()\n",
    "            #Perform one step of gradient descent\n",
    "            optimiser.step()\n",
    "            #Append loss to the general loss for this one epoch\n",
    "            loss_per_epoch += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Loss for Epoch\", epoch, \": \", loss_per_epoch)\n",
    "\n",
    "    time_end = time.time()\n",
    "    time_passed = time_end - time_start\n",
    "    print()\n",
    "    print(f\"{time_passed / 60:.5} minutes ({time_passed:.3} seconds) taken to train the model\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MultiSetNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trying to Build a Network Capable of Processing 64 Subpixels Simultaneously"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialise Inputs Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ccd = CCD()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_pixels = 2\n",
    "num_subpixels = 4\n",
    "max_ccds = 5\n",
    "num_features = 9\n",
    "#df_raw = pd.read_csv('../../bricks_data/dataset_geometric.csv')\n",
    "# Randomly Sampling Pixel Indices from Dataframe\n",
    "#pixel_indices = random.sample(range(len(df_raw)), num_pixels)\n",
    "pix_ids = [1,2]\n",
    "pixel2subpixel_dict = {1:[11,12,13,14], 2:[21,22,23,24]}\n",
    "subpixel2ccd_dict = {11:[111,112,113,114],12:[121,122,123,124,125], 13:[131,132,133,134,135], 14:[141,142,143,144,145],\n",
    "                     21:[211,212,213,214,215],22:[221,222], 23:[231,232,233,234,235], 24:[241,242,243,244,245]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#self.input = -1 * np.ones((self.num_pixels, self.max_ccds, self.num_features))\n",
    "input = np.zeros((num_pixels, num_subpixels, max_ccds, num_features))\n",
    "# Iterate through the pixels\n",
    "print(\"Pixids\", pix_ids)\n",
    "for pix_no, pix in enumerate(pix_ids):\n",
    "\n",
    "    subpix_ids = pixel2subpixel_dict[pix]\n",
    "    subpix_ids = subpix_ids[:num_subpixels]\n",
    "\n",
    "    for subpix_no, subpix in enumerate(subpix_ids):\n",
    "        if subpix not in subpixel2ccd_dict:\n",
    "            continue\n",
    "        subpix_ccds = subpixel2ccd_dict[subpix]\n",
    "        random.shuffle(subpix_ccds)\n",
    "        subpix_ccds = subpix_ccds[:max_ccds]\n",
    "        x = ccd.get_ccds(subpix_ccds)\n",
    "\n",
    "        # Iterate through the CCDs for every pixel\n",
    "        for ccd_no in range(len(subpix_ccds)):\n",
    "            input[pix_no, subpix_no,ccd_no] = x[ccd_no]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialise Lengths Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "var_set_len = True\n",
    "lengths = np.zeros((num_pixels,num_subpixels), dtype=int)\n",
    "print(lengths)\n",
    "if var_set_len:\n",
    "    for pix_no, pix in enumerate(pix_ids):\n",
    "        subpix_ids = pixel2subpixel_dict[pix]\n",
    "        subpix_ids = subpix_ids[:num_subpixels]\n",
    "\n",
    "        for subpix_no, subpix in enumerate(subpix_ids):\n",
    "            if subpix not in subpixel2ccd_dict:\n",
    "                lengths[pix_no, subpix_no] = 0\n",
    "                continue\n",
    "            c = len(subpixel2ccd_dict[subpix])\n",
    "            if c < max_ccds:\n",
    "                lengths[pix_no, subpix_no] = c\n",
    "            else:\n",
    "                lengths[pix_no, subpix_no] = max_ccds\n",
    "\n",
    "else:\n",
    "    lengths.fill(max_ccds)\n",
    "\n",
    "print(lengths)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GetItem Function\n",
    "idx = 0\n",
    "x = torch.from_numpy(input[idx]).float()\n",
    "\n",
    "\n",
    "#l = torch.tensor(self.lengths[idx])\n",
    "l = lengths[idx]\n",
    "\n",
    "print(x.shape)\n",
    "print(l.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# noinspection PyAttributeOutsideInit\n",
    "class MultiSetSequence(Dataset):\n",
    "    \"\"\"Processes and Returns a Dataset of Variable Sized Input Sets of Dimensions\n",
    "    N = Number SubPixels of that are returned --> usually 64\n",
    "    M = Max Size of each Individual Set of CCDs\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pixels= 1000, num_subpixels = 64, max_ccds=30, num_features = 9):\n",
    "\n",
    "        with open('../../bricks_data/mini_multiset.pickle', 'rb') as f:\n",
    "            self.mini_multiset = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "        # Initialise DataSet\n",
    "        self.num_pixels = num_pixels\n",
    "        self.num_features = num_features\n",
    "        self.input = np.zeros((num_pixels, num_subpixels, max_ccds, num_features))\n",
    "        self.lengths = np.zeros((num_pixels,num_subpixels), dtype=int)\n",
    "        self.lrg = np.zeros(num_pixels)\n",
    "        self.elg = np.zeros(num_pixels)\n",
    "        self.qso = np.zeros(num_pixels)\n",
    "\n",
    "        self.initialise_inputs()\n",
    "\n",
    "    def set_targets(self, gal_type):\n",
    "        # Features and inputs:\n",
    "        self.target = None\n",
    "        if gal_type == 'lrg':\n",
    "            self.target = self.lrg\n",
    "        if gal_type == 'elg':\n",
    "            self.target = self.elg\n",
    "        if gal_type == 'qso':\n",
    "            self.target = self.qso\n",
    "        self.scaler_out = preprocessing.MinMaxScaler()\n",
    "        self.target = self.scaler_out.fit_transform(self.target.reshape(-1, 1))\n",
    "\n",
    "    def initialise_inputs(self):\n",
    "        for i, pix in enumerate(self.mini_multiset):\n",
    "            if i >= self.num_pixels:\n",
    "                break\n",
    "            self.input[i] = self.mini_multiset[pix][0]\n",
    "            self.lengths[i] = self.mini_multiset[pix][1]\n",
    "            self.lrg[i] = self.mini_multiset[pix][2]\n",
    "            self.elg[i] = self.mini_multiset[pix][3]\n",
    "            self.qso[i] = self.mini_multiset[pix][4]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_pixels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.input[idx]).float()\n",
    "        #x = x.unsqueeze(0)\n",
    "        y = torch.tensor(self.target[idx, 0]).float()\n",
    "        #print(y.shape)\n",
    "        y = y.unsqueeze(-1)\n",
    "        #print(y.shape)\n",
    "\n",
    "        #l = torch.tensor(self.lengths[idx])\n",
    "        l = self.lengths[idx]\n",
    "\n",
    "        return x, y, l\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "traindata = MultiSetSequence(num_pixels=10)\n",
    "traindata.set_targets('lrg')\n",
    "x, y, l = traindata.__getitem__(3)\n",
    "print(l.shape)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "' TODO\\n1. Train Loop with Batching\\n2. Masking Procedure, need to mask out singular values on top of those that have no CCDs\\n3. How to actually feed real data into the system to see if it can learn\\n'"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepset_layers import InvLinear\n",
    "class MultiSetNet(nn.Module):\n",
    "    def __init__(self, n_features=9, n_output=3, n_subpix = 64, reduction='sum'):\n",
    "        super(MultiSetNet, self).__init__()\n",
    "\n",
    "        # Takes an Input Tensor and applies transformations to last layer --> features\n",
    "        # Output of Feature Layer: Tensor with Max.CCDs elements, which can now be passed to Set Layer\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(n_features, 7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(7, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(5, n_output),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.adder = InvLinear(3, 1, reduction=reduction, bias=True)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_subpix,2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2,1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Invariant Layer Influenced by Code from DPernes, but adapted for the current regression task instead of CNN\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        y = self.feature_extractor(X)\n",
    "        y = self.adder(y, mask=mask)\n",
    "\n",
    "        y = self.mlp(y.T)\n",
    "        return y\n",
    "\n",
    "\"\"\" TODO\n",
    "1. Train Loop with Batching\n",
    "2. Masking Procedure, need to mask out singular values on top of those that have no CCDs\n",
    "3. How to actually feed real data into the system to see if it can learn\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 30, 9])\n",
      "torch.Size([64, 30, 3])\n",
      "tensor([[0.]], grad_fn=<ReluBackward1>)\n"
     ]
    }
   ],
   "source": [
    "net = MultiSetNet()\n",
    "print(x.shape)\n",
    "y = net.forward(x)\n",
    "print(y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Defining Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#Defining Hyperparemeters\n",
    "no_epochs = 100  #very low, but computational power not sufficient for more iterations\n",
    "batch = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Using the Adam Method for Stochastic Optimisation\n",
    "#optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "galaxy_types = ['lrg', 'elg', 'qso']\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "def get_mask(sizes, max_size):\n",
    "    return (torch.arange(max_size).reshape(1, -1).to(sizes.device) < sizes.reshape(-1, 1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "traindata = MultiSetSequence(num_pixels=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GALAXY TYPE:  lrg\n",
      "\n",
      "Loss for Epoch 0 :  21.203732424415648\n",
      "Loss for Epoch 10 :  21.21518995705992\n",
      "Loss for Epoch 20 :  21.21518995705992\n",
      "Loss for Epoch 30 :  21.21518995705992\n",
      "Loss for Epoch 40 :  21.21518995705992\n",
      "Loss for Epoch 50 :  21.21518995705992\n",
      "Loss for Epoch 60 :  21.21518995705992\n",
      "Loss for Epoch 70 :  21.21518995705992\n",
      "Loss for Epoch 80 :  21.21518995705992\n",
      "Loss for Epoch 90 :  21.21518995705992\n",
      "\n",
      "0.19455 minutes (11.7 seconds) taken to train the model\n",
      "\n",
      "GALAXY TYPE:  elg\n",
      "\n",
      "Loss for Epoch 0 :  38.964210975915194\n",
      "Loss for Epoch 10 :  38.91034494712949\n",
      "Loss for Epoch 20 :  38.91034494712949\n",
      "Loss for Epoch 30 :  38.91034494712949\n",
      "Loss for Epoch 40 :  38.91034494712949\n",
      "Loss for Epoch 50 :  38.91034494712949\n",
      "Loss for Epoch 60 :  38.91034494712949\n",
      "Loss for Epoch 70 :  38.91034494712949\n",
      "Loss for Epoch 80 :  38.91034494712949\n",
      "Loss for Epoch 90 :  38.91034494712949\n",
      "\n",
      "0.23638 minutes (14.2 seconds) taken to train the model\n",
      "\n",
      "GALAXY TYPE:  qso\n",
      "\n",
      "Loss for Epoch 0 :  18.97755779325962\n",
      "Loss for Epoch 10 :  18.832214936614037\n",
      "Loss for Epoch 20 :  19.019644618034363\n",
      "Loss for Epoch 30 :  19.00366996228695\n",
      "Loss for Epoch 40 :  18.487545117735863\n",
      "Loss for Epoch 50 :  18.563062861561775\n",
      "Loss for Epoch 60 :  18.284362956881523\n",
      "Loss for Epoch 70 :  18.3128974288702\n",
      "Loss for Epoch 80 :  18.225802645087242\n",
      "Loss for Epoch 90 :  18.286328971385956\n",
      "\n",
      "0.24793 minutes (14.9 seconds) taken to train the model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for gal in galaxy_types:\n",
    "    model = MultiSetNet(n_features=traindata.num_features, reduction='sum').to(device)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(\"GALAXY TYPE: \", gal)\n",
    "    print()\n",
    "    traindata.set_targets(gal_type=gal)\n",
    "\n",
    "    time_start = time.time()\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "        loss_per_epoch = 0\n",
    "        #loading the training data from trainset and shuffling for each epoch\n",
    "        trainloader = torch.utils.data.DataLoader(traindata, batch_size=batch, shuffle=True)\n",
    "\n",
    "        for i, (X, labels, set_sizes) in enumerate(trainloader):\n",
    "            #print(X.shape)\n",
    "            #print(labels.shape)\n",
    "            #print(set_sizes)\n",
    "            #Put Model into train mode\n",
    "            model.train()\n",
    "\n",
    "            #Extract inputs and associated labels from dataloader batch\n",
    "            X = X.squeeze().to(device)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            #set_sizes = set_sizes.to(device)\n",
    "\n",
    "            #mask = get_mask(set_sizes, X.shape[1])\n",
    "\n",
    "            #Predict outputs (forward pass)\n",
    "\n",
    "            # Not yet doing any masking\n",
    "            predictions = model(X)\n",
    "\n",
    "            #Compute Loss\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            #Zero-out the gradients before backward pass (pytorch stores the gradients)\n",
    "            optimiser.zero_grad()\n",
    "            #Backpropagation\n",
    "            loss.backward()\n",
    "            #Perform one step of gradient descent\n",
    "            optimiser.step()\n",
    "            #Append loss to the general loss for this one epoch\n",
    "            loss_per_epoch += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Loss for Epoch\", epoch, \": \", loss_per_epoch)\n",
    "    time_end = time.time()\n",
    "    time_passed = time_end - time_start\n",
    "    print()\n",
    "    print(f\"{time_passed / 60:.5} minutes ({time_passed:.3} seconds) taken to train the model\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training and Comparison"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set-Net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "importlib.reload(sys.modules['utilities'])\n",
    "from utilities import train, multi_train, MultiSetTrainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-Set-Net"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GALAXY TYPE:  lrg\n",
      "\n",
      "Loss for Epoch 0 :  10.053878550264926\n",
      "Loss for Epoch 10 :  10.053878550264926\n",
      "Loss for Epoch 20 :  10.053878550264926\n",
      "Loss for Epoch 30 :  10.053878550264926\n",
      "Loss for Epoch 40 :  10.053878550264926\n",
      "Loss for Epoch 50 :  10.053878550264926\n",
      "Loss for Epoch 60 :  10.053878550264926\n",
      "Loss for Epoch 70 :  10.053878550264926\n",
      "Loss for Epoch 80 :  10.053878550264926\n",
      "Loss for Epoch 90 :  10.053878550264926\n",
      "\n",
      "1.7579 minutes (1.05e+02 seconds) taken to train the model\n",
      "\n",
      "GALAXY TYPE:  elg\n",
      "\n",
      "Loss for Epoch 0 :  11629.811594213443\n",
      "Loss for Epoch 10 :  42.34878332098149\n",
      "Loss for Epoch 20 :  42.36339263490362\n",
      "Loss for Epoch 30 :  42.325412895846654\n",
      "Loss for Epoch 40 :  42.393384256566165\n",
      "Loss for Epoch 50 :  42.373732197336295\n",
      "Loss for Epoch 60 :  42.40096284180902\n",
      "Loss for Epoch 70 :  42.24797562570515\n",
      "Loss for Epoch 80 :  42.28472199212819\n",
      "Loss for Epoch 90 :  42.39563592875416\n",
      "\n",
      "1.7813 minutes (1.07e+02 seconds) taken to train the model\n",
      "\n",
      "GALAXY TYPE:  qso\n",
      "\n",
      "Loss for Epoch 0 :  214.11590725108545\n",
      "Loss for Epoch 10 :  176.07240221574466\n",
      "Loss for Epoch 20 :  176.07240221574466\n",
      "Loss for Epoch 30 :  176.07240221574466\n",
      "Loss for Epoch 40 :  176.07240221574466\n",
      "Loss for Epoch 50 :  176.07240221574466\n",
      "Loss for Epoch 60 :  176.07240221574466\n",
      "Loss for Epoch 70 :  176.07240221574466\n",
      "Loss for Epoch 80 :  176.07240221574466\n",
      "Loss for Epoch 90 :  176.07240221574466\n",
      "\n",
      "1.7694 minutes (1.06e+02 seconds) taken to train the model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_train(1000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 670\n",
      "Test Samples: 330\n"
     ]
    }
   ],
   "source": [
    "trainer = MultiSetTrainer(num_pixels=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GALAXY TYPE:  lrg\n",
      "\n",
      "Loss for Epoch 0 :  100821.82961916682\n",
      "Loss for Epoch 10 :  29.421512088947566\n",
      "Loss for Epoch 20 :  29.905620742014435\n",
      "Loss for Epoch 30 :  29.905620742014435\n",
      "Loss for Epoch 40 :  29.905620742014435\n",
      "Loss for Epoch 50 :  29.905620742014435\n",
      "Loss for Epoch 60 :  29.905620742014435\n",
      "Loss for Epoch 70 :  29.905620742014435\n",
      "Loss for Epoch 80 :  29.905620742014435\n",
      "Loss for Epoch 90 :  29.905620742014435\n",
      "\n",
      "1.2411 minutes (74.5 seconds) taken to train the model\n",
      "\n",
      "GALAXY TYPE:  elg\n",
      "\n",
      "Loss for Epoch 0 :  93.18424911069451\n",
      "Loss for Epoch 10 :  93.18074411245743\n",
      "Loss for Epoch 20 :  93.18087078869576\n",
      "Loss for Epoch 30 :  93.18081105559395\n",
      "Loss for Epoch 40 :  93.18094565512729\n",
      "Loss for Epoch 50 :  93.18083404606296\n",
      "Loss for Epoch 60 :  93.18089803271869\n",
      "Loss for Epoch 70 :  93.18080637308594\n",
      "Loss for Epoch 80 :  93.18084883542906\n",
      "Loss for Epoch 90 :  93.18092544178944\n",
      "\n",
      "1.2404 minutes (74.4 seconds) taken to train the model\n",
      "\n",
      "GALAXY TYPE:  qso\n",
      "\n",
      "Loss for Epoch 0 :  120.18745149873939\n",
      "Loss for Epoch 10 :  120.77336219308927\n",
      "Loss for Epoch 20 :  120.77336219308927\n",
      "Loss for Epoch 30 :  120.77336219308927\n",
      "Loss for Epoch 40 :  120.77336219308927\n",
      "Loss for Epoch 50 :  120.77336219308927\n",
      "Loss for Epoch 60 :  120.77336219308927\n",
      "Loss for Epoch 70 :  120.77336219308927\n",
      "Loss for Epoch 80 :  120.77336219308927\n",
      "Loss for Epoch 90 :  120.77336219308927\n",
      "\n",
      "1.2198 minutes (73.2 seconds) taken to train the model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultiSetNet R^2 for lrg :  -0.9802999802026144.\n",
      "MultiSetNet MSE for lrg :  0.010757701833422987.\n",
      "\n",
      "MultiSetNet R^2 for elg :  -2.142983122931937.\n",
      "MultiSetNet MSE for elg :  0.1886151816532751.\n",
      "\n",
      "MultiSetNet R^2 for qso :  -2.1030073797525928.\n",
      "MultiSetNet MSE for qso :  0.16277207036976923.\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}