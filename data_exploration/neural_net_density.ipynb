{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First attempt at building a Neural Network to learn a non-linear F(s)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing, metrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 268046 entries, 0 to 268045\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   lrg_density        268046 non-null  float64\n",
      " 1   elg_density        268046 non-null  float64\n",
      " 2   qso_density        268046 non-null  float64\n",
      " 3   stellar_density    268046 non-null  float64\n",
      " 4   airmass_galaxy     268046 non-null  float64\n",
      " 5   fwhm_galaxy        268046 non-null  float64\n",
      " 6   ebv_galaxy         268046 non-null  float64\n",
      " 7   ccdnphotom_galaxy  268046 non-null  float64\n",
      " 8   ccdskysb_galaxy_g  268046 non-null  float64\n",
      " 9   ccdskysb_galaxy_r  268046 non-null  float64\n",
      " 10  ccdskysb_galaxy_z  268046 non-null  float64\n",
      " 11  exptime_galaxy_g   268046 non-null  float64\n",
      " 12  exptime_galaxy_r   268046 non-null  float64\n",
      " 13  exptime_galaxy_z   268046 non-null  float64\n",
      " 14  meansky_galaxy_g   268046 non-null  float64\n",
      " 15  meansky_galaxy_r   268046 non-null  float64\n",
      " 16  meansky_galaxy_z   268046 non-null  float64\n",
      " 17  galdepth_galaxy_g  268046 non-null  float64\n",
      " 18  galdepth_galaxy_r  268046 non-null  float64\n",
      " 19  galdepth_galaxy_z  268046 non-null  float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 40.9 MB\n"
     ]
    }
   ],
   "source": [
    "#Preprocess Data\n",
    "df = pd.read_csv('../bricks_data/dataset_geometric.csv')\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining The Dataset Class Inheriting from Torch.dataset to be able to use a dataloader for training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "class DensitySurvey(Dataset):\n",
    "    def __init__(self, df, galaxy_type):\n",
    "        self.data = df[0:10000]\n",
    "        # Extracting Targets and Input\n",
    "        if galaxy_type == \"LRG\":\n",
    "            self.target = self.data['lrg_density'].to_numpy(copy=True)\n",
    "        if galaxy_type == \"ELG\":\n",
    "            self.target = self.data['elg_density'].to_numpy(copy=True)\n",
    "        if galaxy_type == \"QSO\":\n",
    "            self.target = self.data['qso_density'].to_numpy(copy=True)\n",
    "        self.input = self.data.drop(columns=['lrg_density','elg_density','qso_density']).to_numpy(copy=True)\n",
    "\n",
    "        # Scaling\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        self.input = scaler.fit_transform(self.input)\n",
    "        self.target = scaler.fit_transform(self.target.reshape(-1, 1))\n",
    "        print(self.input.shape)\n",
    "        print(self.target.shape)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.input[idx]).float(), torch.tensor(self.target[idx]).float()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 17)\n",
      "(10000, 1)\n",
      "(10000, 17)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../bricks_data/dataset_geometric.csv')\n",
    "train_df, test_df = train_test_split(df, test_size=0.33, random_state=44, shuffle=True)\n",
    "traindata = DensitySurvey(train_df, 'LRG')\n",
    "testdata = DensitySurvey(test_df, 'LRG')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(traindata.__len__())\n",
    "print(testdata.__len__())\n",
    "\n",
    "x,y = traindata.__getitem__(3)\n",
    "\n",
    "print(x.dtype, y.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Models and Hyperparameters\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, n_input_vars = 17, n_output_vars=1):\n",
    "        super().__init__() # call constructor of superclass\n",
    "        self.linear = nn.Linear(n_input_vars, n_output_vars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "model = LinearRegression().to(device)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_feature = 17, n_hidden = 10, n_output = 1):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feature,n_hidden)\n",
    "        #self.fc2 = nn.Linear(n_hidden,n_hidden)\n",
    "        self.predict = nn.Linear(n_hidden,n_output)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.predict(out)\n",
    "        return out\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "model = Net().to(device)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "# Defining Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#Defining Hyperparemeters\n",
    "no_epochs = 250 #very low, but computational power not sufficient for more iterations\n",
    "batch = 1024\n",
    "learning_rate = 0.001\n",
    "\n",
    "#Using the Adam Method for Stochastic Optimisation\n",
    "optimiser = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for Epoch 0 :  0.2225299822166562\n",
      "Loss for Epoch 10 :  0.009243138367310166\n",
      "Loss for Epoch 20 :  0.006941775616724044\n",
      "Loss for Epoch 30 :  0.006417148106265813\n",
      "Loss for Epoch 40 :  0.006184720667079091\n",
      "Loss for Epoch 50 :  0.006093138479627669\n",
      "Loss for Epoch 60 :  0.006262312759645283\n",
      "Loss for Epoch 70 :  0.005947003315668553\n",
      "Loss for Epoch 80 :  0.005873843212611973\n",
      "Loss for Epoch 90 :  0.005843540857313201\n",
      "Loss for Epoch 100 :  0.005823266663355753\n",
      "Loss for Epoch 110 :  0.005770177172962576\n",
      "Loss for Epoch 120 :  0.005731021694373339\n",
      "Loss for Epoch 130 :  0.005684895470039919\n",
      "Loss for Epoch 140 :  0.005716305546229705\n",
      "Loss for Epoch 150 :  0.005664193246047944\n",
      "Loss for Epoch 160 :  0.005634890811052173\n",
      "Loss for Epoch 170 :  0.005607111466815695\n",
      "Loss for Epoch 180 :  0.005618857481749728\n",
      "Loss for Epoch 190 :  0.005576626106631011\n",
      "Loss for Epoch 200 :  0.005566037289099768\n",
      "Loss for Epoch 210 :  0.005584272585110739\n",
      "Loss for Epoch 220 :  0.00552822119789198\n",
      "Loss for Epoch 230 :  0.0055191360879689455\n",
      "Loss for Epoch 240 :  0.005511282972292975\n",
      "\n",
      "0.5991 minutes (35.9 seconds) taken to train the model\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "for epoch in range(no_epochs):\n",
    "    loss_per_epoch = 0\n",
    "\n",
    "    #loading the training data from trainset and shuffling for each epoch\n",
    "    trainloader = torch.utils.data.DataLoader(traindata, batch_size=batch, shuffle = True)\n",
    "\n",
    "    for i, batch_no in enumerate(trainloader, 0):\n",
    "\n",
    "        #Put Model into train mode\n",
    "        model.train()\n",
    "\n",
    "        #Extract inputs and associated labels from dataloader batch\n",
    "        inputs = batch_no[0].to(device)\n",
    "        labels = batch_no[1].to(device)\n",
    "\n",
    "        #Zero-out the gradients before backward pass (pytorch stores the gradients)\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        #Predict outputs (forward pass)\n",
    "        predictions =  model(inputs)\n",
    "\n",
    "        #Compute Loss\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        #Perform one step of gradient descent\n",
    "        optimiser.step()\n",
    "\n",
    "        #Append loss to the general loss for this one epoch\n",
    "        loss_per_epoch += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Loss for Epoch\", epoch, \": \", loss_per_epoch)\n",
    "\n",
    "time_end = time.time()\n",
    "time_passed = time_end - time_start\n",
    "print()\n",
    "print(f\"{time_passed/60:.5} minutes ({time_passed:.3} seconds) taken to train the model\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred = np.array([])\n",
    "testloader = torch.utils.data.DataLoader(testdata, batch_size=batch, shuffle=False)\n",
    "\n",
    "\n",
    "for batch_no in testloader:\n",
    "\n",
    "    #Split dataloader\n",
    "    inputs = batch_no[0].to(device)\n",
    "    labels = batch_no[1].to(device)\n",
    "\n",
    "    #Forward pass through the trained network\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    #Get predictions and append to label array + count number of correct and total\n",
    "    y_pred = np.append(y_pred, outputs.detach().numpy())\n",
    "\n",
    "y_gold = testdata.target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3737632813384124\n",
      "0.008370131576093811\n"
     ]
    }
   ],
   "source": [
    "print(metrics.r2_score(y_gold, y_pred))\n",
    "print(metrics.mean_squared_error(y_gold, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}